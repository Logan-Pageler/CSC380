{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport numpy.random as ra\nimport re\nfrom collections import Counter\nimport traceback, sys, pickle, os\nfrom types import SimpleNamespace\n\n#import ipdb\n\n\n\n################################################################################\n# basic utility functions\n################################################################################\n\ndef nans(*args):\n    ary = np.zeros(*args)\n    ary.fill(np.nan)\n    return ary\n\ndef LoadPickle(fName):\n    \"\"\" load a pickle file. Assumes that it has one dictionary object that points to\n many other variables.\"\"\"\n    if type(fName) == str:\n        try:\n            fp = open(fName, 'rb')\n        except:\n            print(\"Couldn't open %s\" % (fName))\n            traceback.print_exc(file=sys.stderr)\n    else:\n        fp = fName\n    try:\n        ind = pickle.load(fp)\n        fp.close()\n        return ind\n    except:\n        print(\"Couldn't read the pickle file\", fName)\n        traceback.print_exc(file=sys.stderr)\n\ndef SavePickle(filename, var, protocol=2):\n    try:\n        with open(filename, 'wb') as f:\n            pickle.dump(var, f, protocol=protocol)\n        statinfo = os.stat(filename,)\n        if statinfo:\n            print(\"Wrote out\", statinfo.st_size, \"bytes to\", \\\n                filename)\n    except:\n        print(\"Couldn't pickle the file\", filename)\n        traceback.print_exc(file=sys.stderr)\n\n\ndef printExpr(expr, bPretty=True):\n    \"\"\" Print the local variables in the caller's frame.\"\"\"\n    from pprint import pprint\n    import inspect\n    frame = inspect.currentframe()\n    try:\n        loc = frame.f_back.f_locals\n        glo = frame.f_back.f_globals\n        print(expr, '= ', end=' ') \n        if (bPretty):\n            pprint(eval(expr, glo, loc))\n        else:\n            print((eval(expr, glo, loc))); \n    finally:\n\n        del frame\n\nfrom datetime import datetime\n\ndef tic():\n    \"\"\"\n    equivalent to Matlab's tic. It start measuring time.\n    returns handle of the time start point.\n    \"\"\"\n    global gStartTime\n    gStartTime = datetime.utcnow()\n    return gStartTime\n\ndef toc(prev=None):\n    \"\"\"\n    get a timestamp in seconds. Time interval is from previous call of tic() to current call of toc().\n    You can optionally specify the handle of the time ending point.\n    \"\"\"\n    if prev==None: prev = gStartTime\n    return (datetime.utcnow() - prev).total_seconds()\n\n################################################################################\n# extracting features\n################################################################################\ng_out_of_char_vocab = set()\ndef calc_char_unigram_features(s, vocab):\n    global g_out_of_char_vocab\n    paired = zip(vocab, np.arange(len(vocab)))\n    vocab_to_idx = dict(paired)\n\n    ss = s.strip().lower()\n    res = [list(ss[ele.start():ele.end()]) for ele in re.finditer(r'[^ \\t\\n\\r\\f\\v,.;:()]+', ss)]\n\n    # flatten it out for unigram\n    flattened = [y for x in res for y in x]\n    cntr = Counter(flattened)\n    vec = np.zeros(len(vocab))\n    for k,v in cntr.items():\n        if (k not in vocab_to_idx):\n            g_out_of_char_vocab.add(k)\n        else:\n            vec[vocab_to_idx[k]] = v\n\n    if (vec.sum() != 0.0):\n        vec /= vec.sum()\n    return vec\n\ndef calc_char_bigrams(text):\n    ### TODO insert code here.\n    text = re.sub(r\"[!,\\.:;?]\", \" \", text)\n\n    words = text.split()\n\n    bigrams=[]\n\n    for word in words:\n        bigrams.append(word[0])\n        for i in range(0, len(word) -1):\n            bigrams.append(word[i:i+2])\n        bigrams.append(word[-1])\n    \n    return bigrams\n\ndef calc_char_bigram_features(s, vocab, vocab_set, vocab_inv):\n    ss = ' ' + s.strip().lower() + ' '\n    bigrams = list(filter(lambda x: x in vocab_set, calc_char_bigrams(ss)))\n\n    cnt = Counter(bigrams)\n    #vec = np.array( [cnt[k] for k in vocab] , dtype=float)\n    vec = np.zeros(len(vocab))\n    for (k,v) in cnt.items():\n        vec[vocab_inv[k]] = v\n\n    total = vec.sum()\n\n    if (total != 0.0):\n        vec /= total\n\n    return vec\n\n################################################################################\n# for data processing\n################################################################################\n\nclass DataWrapper:\n    def __init__(self, df_features, df_pnotes, df_train, df_test):\n        self.df_features = df_features\n        self.df_pnotes = df_pnotes\n        self.df_train = df_train\n        self.df_test  = df_test\n        self.efeatures = []\n        self.n_efeatures_list = []\n        for i_case in range(10):\n            self.efeatures.append( np.unique(df_features[df_features.case_num == i_case].feature_num).tolist() )\n            self.n_efeatures_list.append( len(self.efeatures[i_case]) )\n        self.n_efeatures_list = np.array(self.n_efeatures_list)\n\n        #- obtain pnotes length\n        # go through pnotes with valid pn_num.\n        self.pn_history_len = dict()\n        for i in range(len(self.df_pnotes)):\n            my = self.df_pnotes.iloc[i]\n            self.pn_history_len[my.pn_num] = len(my.pn_history)\n\n    def get_efeatures(self):\n        return self.efeatures\n\n    def get_n_efeatures_list(self):\n        return self.n_efeatures_list\n\n    def get_ground_truth(self, case_num, pn_num):\n        \"\"\"\n        returns the ground truth in the form of logical matrix of size (n_efeatures, length of pn_history for pn_num)\n        \"\"\"\n        n_efeatures = self.n_efeatures_list[case_num] \n        my_efeatures = self.efeatures[case_num]\n\n        true_mat = np.zeros((n_efeatures, self.pn_history_len[pn_num]), dtype=bool)\n        cond_pn_num = (self.df_train.pn_num == pn_num)\n\n        for i_ef, efeature_num in enumerate(my_efeatures):\n            loc = self.df_train[cond_pn_num \\\n                              & (self.df_train.feature_num == efeature_num)].location\n            for (from_,to_) in location_str_to_ary(loc.tolist()[0]):\n                true_mat[i_ef, from_:to_] = True\n\n        return true_mat\n\n    def get_case_num(self,pn_num):\n        return self.df_pnotes[self.df_pnotes.pn_num == pn_num].case_num.tolist()[0]\n\n    def get_pn_history(self, pn_num):\n        return self.df_pnotes[self.df_pnotes.pn_num == pn_num].pn_history.tolist()[0]\n    pass\n\ndef loc_is_in(loc, true_loc_mat):\n    \"\"\"\n    true_loc is n by 2\n    \"\"\"\n    ret_val = False\n    for i in range(len(true_loc_mat)):\n        if (loc[0] >= true_loc_mat[i][0] and loc[1] <= true_loc_mat[i][1]):\n            ret_val = True\n            break\n    return ret_val\n\ndef get_word_loc_ary(text):\n    \"\"\"\n    a list of tuple (start_index, end_index) indicating each word location; note we follow python's standard indexing, so the word starts from start_index but ends at end_index-1\n    \"\"\"\n    return np.array([(ele.start(), ele.end()) for ele in re.finditer(r'[^ \\t\\n\\r\\f\\v,.;:()]+', text.strip())])\n\n\ndef get_loc_string(pred_ary):\n    \"\"\"\n        In [32]: get_loc_string(np.array([1,0,1,1,0,1],dtype=bool))\n        Out[32]: '0 1;2 4;5 6'\n    \"\"\"    \n    switch = False\n    locations = []\n    n = len(pred_ary)\n    for i in range(-1,n):\n        # v: value at i\n        # vv: value at i+1\n        if (i == -1):\n            v = False # sentinel\n        else:\n            v = pred_ary[i]\n\n        if (i == n - 1):\n            vv = False # sentinel\n        else:\n            vv = pred_ary[i+1]\n\n        if (np.logical_xor(v,vv)):\n            locations.append(i+1)\n\n    assert len(locations) % 2 == 0\n    loc = np.array(locations)\n    loc = loc.reshape(-1,2)\n    return ';'.join(['%d %d' % (row[0], row[1])for row in loc])\n\ndef location_str_to_ary(loc):\n    \"\"\"\n    In [102]: location_str_to_ary(\"['595 724']\")\n    Out[102]: [[595, 724]]\n\n    In [100]: location_str_to_ary(\"['595 724', '652 661']\")\n    Out[100]: [[595, 724], [652, 661]]\n\n    In [57]: location_str_to_ary(\"['595 724', '652 661; 665 670']\")\n    Out[57]: [[595, 724], [652, 661], [665, 670]]\n\n    In [101]: location_str_to_ary(\"[]\")\n    Out[101]: []\n    \"\"\"    \n    myloc = \"', '\".join(loc.split(';'))\n    myloc = eval(myloc)\n    # later, I may need to use binary array representation..\n    #     tmp = [[int(j) for j in pair_str.split()] for pair_str in myloc]\n    #     maxval = max([row[1] for row in tmp])\n    # \n    #     bin_ary = np.zeros(maxval, dtype=bool)\n    #     for intv in range(len(tmp)):\n    #         bin_ary[intv[0]:intv[1]] = True\n    # \n    #     bin_ary\n\n    return [[int(j) for j in pair_str.split()] for pair_str in myloc]\n\n################################################################################\n# for feature extraction\n################################################################################\n\ndef myfeatures(pn_history, cur_loc_mat, char_vocab):\n    cur_loc_all = [cur_loc_mat[0,0], cur_loc_mat[-1,1]]\n    \n    fvec_names = []\n    fvec = []\n\n    fvec_names.append( 'n_chars' )\n    fvec.append( np.sum([loc[1] - loc[0] for loc in cur_loc_mat]) )\n\n    fvec_names.append( 'n_words' )\n    fvec.append( cur_loc_mat.shape[0] )\n\n    fvec_names += ['char_unigram_' + c for c in char_vocab]\n    text = pn_history[cur_loc_all[0]:cur_loc_all[1]]\n    fvec += calc_char_unigram_features(text, char_vocab).tolist()\n\n    return fvec_names, fvec\n\n\ndef myfeatures_v02(pn_history, cur_loc_mat, char_vocab, bigram_vocab, bigram_vocab_set, bigram_vocab_inv):\n    cur_loc_all = [cur_loc_mat[0,0], cur_loc_mat[-1,1]]\n    \n    fvec_names = []\n    fvec = []\n\n    fvec_names.append( 'n_chars' )\n    fvec.append( np.sum([loc[1] - loc[0] for loc in cur_loc_mat]) )\n\n    fvec_names.append( 'n_words' )\n    fvec.append( cur_loc_mat.shape[0] )\n\n    text = pn_history[cur_loc_all[0]:cur_loc_all[1]]\n\n    fvec_names += ['char_unigram_' + c for c in char_vocab]\n    fvec += calc_char_unigram_features(text, char_vocab).tolist()\n\n    fvec_names += ['char_bigram_' + repr(c) for c in bigram_vocab]\n    fvec += calc_char_bigram_features(text, bigram_vocab, bigram_vocab_set, bigram_vocab_inv).tolist()\n\n    return fvec_names, fvec\n\nclass MicroDataExtractor:\n    def __init__(self, W, char_vocab, feature_extractor):\n        \"\"\"\n        feature_extractor must be a function that takes (pn_history, cur_loc_mat) and output the feature vector representation\n        \"\"\"\n        self.W = W\n        self.char_vocab = char_vocab\n        assert feature_extractor is not None\n        self.feature_extractor = feature_extractor\n\n        pass\n\n    def extract_micro_data(self, pn_num, pn_history, true_mat=None):\n        \"\"\"\n           true_mat: np.array with size (n_efeatures, length of pn_history)\n                     if None, we do not extract the label (this is true for test data) \n        \"\"\"\n        out_table = []\n        loc_ary = get_word_loc_ary(pn_history)\n        for i_loc in range(loc_ary.shape[0]):\n            # FIXME could use the generator pattern\n            # for w in range \n            for w in range(1, 1+self.W):\n                if (i_loc+w > loc_ary.shape[0]):\n                    break\n                cur_loc_mat = loc_ary[i_loc:i_loc+w,:]\n                cur_loc_all = [cur_loc_mat[0,0], cur_loc_mat[-1,1]]\n\n                #- obtain the label\n                label = None\n                if (true_mat is not None):\n                    label = -1\n                    multi_label = []\n                    for i_ef in range(true_mat.shape[0]):\n                        if (all(true_mat[i_ef,cur_loc_all[0]:cur_loc_all[1]])):\n                            v = 1\n                            label = i_ef\n                        else:\n                            v = -1\n                        multi_label.append( v )\n                    assert (np.sum(multi_label != -1) <= 1)\n\n                #- compute the feature vector\n                fvec_names, fvec = self.feature_extractor(pn_history, cur_loc_mat)\n                fvec = [np.float32(x) for x in fvec]        # 32 bits to save the space\n\n                out_row = [pn_num, cur_loc_all[0], cur_loc_all[1]] + fvec + [label]\n                col_names = ['pn_num', 'loc_from', 'loc_to'] + fvec_names + ['label']\n                out_table.append(out_row)\n                pass\n            pass\n        return out_table, col_names\n        \n        \n\n        \n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-06T05:36:39.687892Z","iopub.execute_input":"2023-05-06T05:36:39.688308Z","iopub.status.idle":"2023-05-06T05:36:39.741539Z","shell.execute_reply.started":"2023-05-06T05:36:39.688275Z","shell.execute_reply":"2023-05-06T05:36:39.740211Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom tqdm import tqdm\nfrom types import SimpleNamespace\n\nopt = SimpleNamespace()\nopt.kaggle_submission = True\nopt.use_bigram = True #- FIXME: you can switch to bigram \nif (opt.kaggle_submission):\n    opt.prefix = \"/kaggle/input/nbme-score-clinical-patient-notes/\"\nelse:\n    opt.prefix = \"./data/\"\n    import ipdb\n    import mylib\n    import importlib\n    importlib.reload(mylib)\n    from mylib import *\nprintExpr('opt')\n\ndataopt = SimpleNamespace()\ndataopt.W = 5\ndataopt.bigramcutoff = 30\nprintExpr('dataopt')\n\nprint(\"Loading data... \", end='')\ntic()\ndf_efeatures = pd.read_csv(opt.prefix+\"features.csv\")\ndf_pnotes    = pd.read_csv(opt.prefix+\"patient_notes.csv\")\ndf_train     = pd.read_csv(opt.prefix+\"train.csv\")\ndf_test      = pd.read_csv(opt.prefix+\"test.csv\") \ndataw = DataWrapper(df_efeatures, df_pnotes, df_train, df_test)\nprint(\"Done (%.2fs)\"% toc())\n\n#--- compute the distribution of the number of words in 'annotation'\nrows = df_train.annotation\nlen_ary = []\nfor row in rows:\n    str_ary = eval(row)\n    for a_str in str_ary:\n        len_ary.append( len(a_str.split()) )\n\nfrom collections import Counter\ncc = Counter(len_ary);\nfor k in sorted(cc.keys()):\n    print(\"%02d: %6d\" % (k, cc[k]))\n\nn_efeatures_list = dataw.get_n_efeatures_list()\nprintExpr(\"n_efeatures_list\")\n\nn_cases = 10\n\n#--- thedata : n_case by n_substring by n_features\n#- each row, case_num, pn_num, loc_from, loc_to, ...\n\ng_char_vocab = list('abcdefghijklmnopqrstuvwxyz0123456789-\"\\'/@')\n\n\nif (opt.use_bigram):\n    #------ first scan\n    #- the goal is to collect the bigrams and compute the occurrences of each bigram throughout. \n    print('\\n#----- first scan\\n')\n    from collections import Counter\n    import re\n\n    g_counter = None\n    for case_num in range(n_cases):\n        print('#--- case_num = %5d' % case_num)\n        my_efeatures = np.unique(df_efeatures[df_efeatures.case_num == case_num].feature_num)\n        n_efeatures = len(my_efeatures)\n        my_train = df_train[df_train.case_num == case_num]\n        my_pn_nums = my_train.pn_num\n\n        #--- patient notes that are annotated & case_num = case_num\n        my_pnotes = df_pnotes[(df_pnotes.case_num == case_num) & (df_pnotes.pn_num.isin(my_pn_nums)) ]\n        my_pnotes.reset_index(inplace=True)\n\n        out_table = []\n        for i_row in tqdm(range(len(my_pnotes))):\n            row = my_pnotes.loc[i_row]\n            pn_history = row.pn_history\n            pn_num = row.pn_num\n            \n            sub = 0\n            hist = \"\"\n            length = len(pn_history)\n            for i, c in enumerate(pn_history):\n                if c.isdigit():\n                    if i+1 < length and pn_history[i+1].isspace():\n                        hist += pn_history[sub:i+1]\n                        sub=i+2\n            \n            if(sub < length):\n                hist += pn_history[sub:length]\n            \n            cnt = Counter(calc_char_bigrams(hist))\n            if (g_counter is None):\n                g_counter = cnt\n            else:\n                g_counter += cnt\n            pass\n        pass\n\n    bb = [[k,v] for k,v in g_counter.items()]\n    bb = sorted(bb, key=lambda x: x[1])[::-1]\n\n    print('first 10 bigrams:')\n    cnt = 0\n    for k,v in bb:\n        print(\"%-7s : %7d\" % (repr(k),v))\n        cnt += 1\n        if (cnt >= 10):\n            break\n\n    bigram_vocab = []\n    for (k,v) in bb:\n        if (v >= dataopt.bigramcutoff):\n            bigram_vocab.append( k )\n\n    bigram_vocab = sorted(bigram_vocab) \n\n#----- second scan\n\nif opt.use_bigram:\n    bigram_vocab_inv = dict(zip(bigram_vocab, range(len(bigram_vocab))))\n    bigram_vocab_set = set(bigram_vocab)\n    feature_extractor = lambda pn_history, cur_loc_mat: myfeatures_v02(pn_history, cur_loc_mat, g_char_vocab, bigram_vocab, bigram_vocab_set, bigram_vocab_inv)\nelse:\n    feature_extractor = lambda pn_history, cur_loc_mat: myfeatures(pn_history, cur_loc_mat, g_char_vocab)\n\nmde = MicroDataExtractor(dataopt.W, g_char_vocab, feature_extractor)\npdata = []\nfor case_num in range(n_cases):\n    print('#--- case_num = %5d' % case_num)\n    my_efeatures = np.unique(df_efeatures[df_efeatures.case_num == case_num].feature_num)\n    n_efeatures = len(my_efeatures)\n    my_train = df_train[df_train.case_num == case_num]\n    my_pn_nums = my_train.pn_num\n\n    #--- patient notes that are annotated & case_num = case_num\n    my_pnotes = df_pnotes[(df_pnotes.case_num == case_num) & (df_pnotes.pn_num.isin(my_pn_nums)) ]\n    my_pnotes.reset_index(inplace=True)\n\n    out_table = []\n    for i_row in tqdm(range(len(my_pnotes))):\n        row = my_pnotes.loc[i_row]\n        pn_history = row.pn_history\n        pn_num = row.pn_num\n\n        # extract ground truth for each feature\n        true_mat = dataw.get_ground_truth(case_num, pn_num)\n\n        my_out_table, col_names = mde.extract_micro_data(pn_num, pn_history, true_mat)\n        out_table += my_out_table\n        pass\n        \n    pdata.append( pd.DataFrame(out_table, columns=col_names) )\n    pass\n\nout = SimpleNamespace()\nout.pdata = pdata\nout.dataopt = dataopt\nout.char_vocab = g_char_vocab\nif (opt.use_bigram):\n    out.bigram_vocab = bigram_vocab\n\nif (not opt.kaggle_submission):\n    SavePickle(\"pdata_v01_RENAME.pkl\", out) # c0 for case_num = 0\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-05-06T05:36:46.090336Z","iopub.execute_input":"2023-05-06T05:36:46.090786Z","iopub.status.idle":"2023-05-06T05:36:46.116458Z","shell.execute_reply.started":"2023-05-06T05:36:46.090749Z","shell.execute_reply":"2023-05-06T05:36:46.114419Z"},"trusted":true},"execution_count":3,"outputs":[{"traceback":["\u001b[0;36m  Cell \u001b[0;32mIn[3], line 89\u001b[0;36m\u001b[0m\n\u001b[0;31m    hist = \"\"\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"],"ename":"IndentationError","evalue":"unexpected indent (3683672950.py, line 89)","output_type":"error"}]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom sklearn import tree\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom tqdm import tqdm\nfrom types import SimpleNamespace\n\nopt = SimpleNamespace()\nopt.kaggle_submission = True\nopt.use_bigram = True\nopt.debug = False\nif (opt.kaggle_submission):\n    opt.prefix = \"/kaggle/input/nbme-score-clinical-patient-notes/\"\nelse:\n    opt.prefix = \"./\"\n    import ipdb\n    import mylib\n    import importlib\n    importlib.reload(mylib)\n    from mylib import *\nprintExpr('opt')\n\nn_cases = 10\n\n#--- load data\nprint(\"Loading data... \", end='')\ntic()\ndf_efeatures = pd.read_csv(opt.prefix+\"features.csv\")\ndf_pnotes    = pd.read_csv(opt.prefix+\"patient_notes.csv\")\ndf_train     = pd.read_csv(opt.prefix+\"train.csv\")\ndf_test      = pd.read_csv(opt.prefix+\"test.csv\") \ndataw = DataWrapper(df_efeatures, df_pnotes, df_train, df_test)\nprint(\"Done (%.2fs)\"% toc())\n\nn_efeatures_list = [len(df_efeatures[df_efeatures.case_num == i]) for i in range(10)]\nprintExpr(\"n_efeatures_list\")\n\n# #---------------------------\nif (not opt.kaggle_submission):\n    # FIXME: change the file\n    out = LoadPickle('pdata_v01.pkl')\n\npdata = out.pdata\nchar_vocab = out.char_vocab\nif (opt.use_bigram):\n    bigram_vocab = out.bigram_vocab\ndataopt = out.dataopt\n\n\nra.seed(39)\n\n#---- let's train\nclf_list = []\nfor (case_num, mypdata) in enumerate(pdata):\n    print('#--- case_num = %5d' % case_num)\n\n    trainX = mypdata.iloc[:,3:-1]\n    trainY = mypdata.iloc[:,-1]\n\n    # FIXME choose your classifier here\n    clf = RandomForestClassifier(ccp_alpha=0.00001)\n    if opt.debug:\n        clf = clf.fit(trainX[:400],trainY[:400])\n    else:\n        clf = clf.fit(trainX,trainY)\n    clf_list.append(clf)\n\n#---- let's test\npredictions = []\n\nif opt.use_bigram:\n    bigram_vocab_inv = dict(zip(bigram_vocab, range(len(bigram_vocab))))\n    bigram_vocab_set = set(bigram_vocab)\n    feature_extractor = lambda pn_history, cur_loc_mat: myfeatures_v02(pn_history, cur_loc_mat, char_vocab, bigram_vocab, bigram_vocab_set, bigram_vocab_inv)\nelse:\n    feature_extractor = lambda pn_history, cur_loc_mat: myfeatures(pn_history, cur_loc_mat, char_vocab)\nmde = MicroDataExtractor(dataopt.W, char_vocab, feature_extractor)\n\npn_num_ary = np.unique(df_test.pn_num)\nanswers = dict()\nfor (i,pn_num) in enumerate(pn_num_ary):\n    pn_history = dataw.get_pn_history(pn_num)\n\n    my_out_table, col_names = mde.extract_micro_data(pn_num, pn_history)\n\n    test = pd.DataFrame(my_out_table, columns = col_names)\n    case_num = dataw.get_case_num(pn_num)\n    my_efeatures = dataw.get_efeatures()[case_num]\n    my_n_efeatures = len(my_efeatures)\n    my_clf = clf_list[case_num]\n\n    testX = test.iloc[:,3:-1]\n\n    #- predict\n    predY = my_clf.predict(testX)\n    pred_mat = np.zeros((my_n_efeatures,len(pn_history)), dtype=bool)\n    pred_mat[:,:] = False\n\n    #- make prediction for each phrase.\n    for i in range(len(test)):\n        pred = predY[i]\n        if (pred != -1):\n            from_ = test.iloc[i].loc_from\n            to_   = test.iloc[i].loc_to\n            pred_mat[pred, from_:to_] = True\n\n    #- go from pred_mat to to loc_string\n    pn_answers = dict()\n    for i_ef, efeature_num in enumerate(my_efeatures):\n        pn_answers[efeature_num] = get_loc_string(pred_mat[i_ef,:])\n\n    answers[pn_num] = pn_answers\n\n#--- let's write out the answer\npredictions = []\nfor i in range(len(df_test)):\n    row = df_test.iloc[i]\n    pn_num = row.pn_num\n    case_num = dataw.get_case_num(pn_num)\n    predictions.append(answers[pn_num][row.feature_num])\n    \noutput = pd.DataFrame({'id':df_test.id, 'location':predictions})\noutput.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-05-06T05:36:26.706098Z","iopub.execute_input":"2023-05-06T05:36:26.706731Z","iopub.status.idle":"2023-05-06T05:36:29.013087Z","shell.execute_reply.started":"2023-05-06T05:36:26.706697Z","shell.execute_reply":"2023-05-06T05:36:29.011288Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m     importlib\u001b[38;5;241m.\u001b[39mreload(mylib)\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmylib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m---> 28\u001b[0m \u001b[43mprintExpr\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     30\u001b[0m n_cases \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m#--- load data\u001b[39;00m\n","\u001b[0;31mNameError\u001b[0m: name 'printExpr' is not defined"],"ename":"NameError","evalue":"name 'printExpr' is not defined","output_type":"error"}]}]}